{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWniITdkHa2NpU8ESHo8NA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1rd0/VK_test/blob/main/VK_TEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install faker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZswE73g4bwb",
        "outputId": "008f3576-0611-4405-f6dc-4f496a72751f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (29.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "def generate_test_data(num_rows):\n",
        "    data = {\n",
        "        'email': [fake.email() for _ in range(num_rows)],\n",
        "        'action': [random.choice(['CREATE', 'READ', 'UPDATE', 'DELETE']) for _ in range(num_rows)],\n",
        "        'dt': [fake.date_between(start_date='-1y', end_date='today') for _ in range(num_rows)]\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "df = generate_test_data(1000)\n",
        "\n",
        "df.to_csv('test_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "jf7e-jlk46UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Log Processing\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/content/test_data.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "9CWvKnok44Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orkTuVUp90xd",
        "outputId": "21c451e8-f1ce-44e9-f3f4-70faec4defe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+----------+\n",
            "|               email|action|        dt|\n",
            "+--------------------+------+----------+\n",
            "|eharrison@example...|CREATE|2024-07-21|\n",
            "|leonardchristophe...|CREATE|2024-07-20|\n",
            "|dorothy78@example...|UPDATE|2024-01-29|\n",
            "| kelly86@example.net|CREATE|2024-05-21|\n",
            "|heather66@example...|DELETE|2023-10-09|\n",
            "| peter93@example.org|  READ|2023-12-20|\n",
            "|ronniegomez@examp...|CREATE|2023-10-23|\n",
            "|   ososa@example.net|CREATE|2024-06-05|\n",
            "|derekdavis@exampl...|DELETE|2024-08-30|\n",
            "|drodgers@example.net|  READ|2023-09-25|\n",
            "| hwright@example.org|CREATE|2024-07-09|\n",
            "|luisjenkins@examp...|  READ|2024-08-11|\n",
            "|katherineball@exa...|CREATE|2024-06-28|\n",
            "| wmartin@example.org|CREATE|2024-03-24|\n",
            "|wendymunoz@exampl...|DELETE|2024-08-22|\n",
            "| kylie23@example.net|CREATE|2024-02-26|\n",
            "|   vking@example.org|DELETE|2024-08-11|\n",
            "|imcbride@example.net|DELETE|2024-07-02|\n",
            "|taylor51@example.com|  READ|2024-05-19|\n",
            "|kingsamantha@exam...|  READ|2024-08-05|\n",
            "+--------------------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, date_sub\n",
        "\n",
        "def calculate_weekly_aggregate(date_str):\n",
        "    #читаем запрос\n",
        "    target_date = pd.to_datetime(date_str)\n",
        "\n",
        "    start_date = target_date - pd.Timedelta(days=7)\n",
        "    end_date = target_date - pd.Timedelta(days=1)\n",
        "\n",
        "    filtered_df = df.filter((col(\"dt\") >= start_date.strftime('%Y-%m-%d')) &\n",
        "                             (col(\"dt\") <= end_date.strftime('%Y-%m-%d')))\n",
        "\n",
        "\n",
        "    aggregated_df = filtered_df.groupBy(\"email\", \"action\").agg(count(\"action\").alias(\"count\"))\n",
        "    pivoted_df = aggregated_df.groupBy(\"email\").pivot(\"action\").agg(count(\"count\")).na.fill(0)\n",
        "\n",
        "    # Переименовываем колонки\n",
        "    pivoted_df = pivoted_df.withColumnRenamed(\"CREATE\", \"create_counter\") \\\n",
        "                           .withColumnRenamed(\"READ\", \"read_counter\") \\\n",
        "                           .withColumnRenamed(\"UPDATE\", \"update_counter\") \\\n",
        "                           .withColumnRenamed(\"DELETE\", \"delete_counter\")\n",
        "\n",
        "\n",
        "    output_file = f\"/content/output/{target_date.strftime('%Y-%m-%d')}.csv\"\n",
        "    pivoted_df.coalesce(1).write.csv(output_file, mode=\"overwrite\", header=True)\n",
        "\n",
        "    print(f\"Aggregated data saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "AFF3s12j45PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#директория для выходных файлов\n",
        "os.makedirs(\"/content/output\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "LeXXiQah9fc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#выбираем нужную дату"
      ],
      "metadata": {
        "id": "3ryBw9uA-DD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_weekly_aggregate(\"2024-09-16\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKMCjcg09i9T",
        "outputId": "7f211672-f4e4-4865-d356-bbc4166c10ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated data saved to /content/output/2024-09-16.csv\n"
          ]
        }
      ]
    }
  ]
}